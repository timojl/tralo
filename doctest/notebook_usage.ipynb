{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "from tralo import TrainingLogger\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from os.path import expanduser\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "configs = [\n",
    "    dict(a=1, b='Muh'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_metric(model):\n",
    "    return [['abc', 0.3]]\n",
    "\n",
    "with TrainingLogger(metric=(random_metric, 10)) as logger:\n",
    "\n",
    "    for i_epoch in range(2):\n",
    "        for i_iter in range(100):\n",
    "\n",
    "            fake_loss = np.random.normal() + 3 - (i_epoch*100 + i_iter) / 1000\n",
    "            logger(i_epoch*100 + i_iter, loss=fake_loss)\n",
    "\n",
    "        # let's assume we evaluate the metric only once per epoch\n",
    "        fake_metric = np.random.normal() + (i_epoch*100 + i_iter) / 1000\n",
    "        logger(i_epoch*100 + i_iter, fake_metric=fake_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(x for k in logger.metric_values.values() for x in k.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from os.path import expanduser\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as nnf\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mnist = MNIST(expanduser('~/datasets'), transform=tf, download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(mnist, batch_size=16)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 4, 3),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(4, 8, 3),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(8, 16, 3),\n",
    "    nn.MaxPool2d(3),\n",
    "    nn.Conv2d(16, 10, 1),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File-based version\n",
    "\n",
    "This will save statistics in `logs/mnist_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TrainingLogger(f'mnist_run2', interval=50, utilization_iters=400, grad_weights=[('second_layer', model[2])], grad_interval=20) as logger:\n",
    "    i = 0\n",
    "\n",
    "    # make sure we write utility to disk more often\n",
    "    logger.utilization_process.step_interval.value = 5\n",
    "\n",
    "    for i_epoch in range(1):\n",
    "        for data_x, data_y in data_loader:\n",
    "\n",
    "            pred = model(data_x)\n",
    "            loss = nnf.cross_entropy(pred, data_y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            logger.iter(i=i, loss=loss)\n",
    "            i += 1\n",
    "            \n",
    "            if i > 500:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.plots() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TrainingLogger(interval=50, utilization_iters=500, grad_weights=[('second_layer', model[2])], grad_interval=20) as logger:\n",
    "    i = 0\n",
    "    for i_epoch in range(1):\n",
    "        for data_x, data_y in data_loader:\n",
    "\n",
    "            # make sure we write utility to disk more often\n",
    "            logger.utilization_process.step_interval.value = 20\n",
    "\n",
    "            pred = model(data_x)\n",
    "            loss = nnf.cross_entropy(pred, data_y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            logger.iter(i=i, loss=loss)\n",
    "            i += 1\n",
    "            \n",
    "            if i > 500:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.plots()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "800ed241f7db2bd3aa6942aa3be6809cdb30ee6b0a9e773dfecfa9fef1f4c586"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('env2': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
